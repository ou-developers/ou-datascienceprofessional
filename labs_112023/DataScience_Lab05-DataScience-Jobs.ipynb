{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</font>\n",
    "\n",
    "***\n",
    "\n",
    "# <font> Using Data Science Jobs to Automate Model Building and Training</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\"> <font color=\"teal\"></font></p>\n",
    "\n",
    "***\n",
    "\n",
    "## Overview\n",
    "\n",
    "Notebook sessions are not ideal for long-running operations. Generally, notebooks sessions use relatively small compute shapes and you are running one at a time. Further, they are designed to be interactive and this may not always be practical. The Data Science Jobs Service is designed to execute arbitrary scripts in a headless manner. This means they run without a display. A common use case for data scientists is to train a model using a job. When a job is executed, the underlying resources are provisioned and then the compute instance is prepared with the conda environment that it needs along with a script. The script is then run and the resources are shut down when the script ends. Therefore, you only pay for the compute that you use. It also allows you to select the compute instance size based on the performance that is needed.\n",
    "\n",
    "This notebook demonstrates how to create a script, configure logs so that the output can be monitored, and create a job and an associated job run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please select the  published conda envionment data-science-gmlv1_0_v1 before proceeding further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "from ads.common.oci_logging import OCILogGroup, OCILog\n",
    "from ads.jobs import Job, DataScienceJob, ScriptRuntime\n",
    "\n",
    "# Use resource principal to authenticate with the Data Science Jobs API: \n",
    "ads.set_auth(auth=\"resource_principal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  O  o-o   o-o\n",
      " / \\ |  \\ |\n",
      "o---o|   O o-o\n",
      "|   ||  /     |\n",
      "o   oo-o  o--o\n",
      "\n",
      "ads v2.8.11\n",
      "oci v2.114.0\n",
      "ocifs v1.1.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads.hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Script\n",
    "\n",
    "This notebook demonstrates how to create a Job and Job Run but using an example where a model is trained. The normal use case for using a Job to train a model is when the model takes a significant amount of time to train. In this notebook, the model only takes a few seconds to train but the goal is to demonstrate the steps, not train a production-grade model.\n",
    "\n",
    "The first step is to create the script that is executed as part of the job. This script will be stored the training script in a job artifact folder (`./job-artifact`) and performs the following actions:\n",
    "\n",
    "* Pulls the data from Object storage. You must be in the Ashburn region.\n",
    "* Uses ADS to perform automatic data transformation.\n",
    "* Creates an sklearn pipeline object.\n",
    "* Trains a random forest classifier.\n",
    "* Uses the SklearnModel to prepare and save the model to the Model Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to artifact directory for my sklearn model: \n",
    "job_artifact_location = os.path.expanduser('./job-artifact/')\n",
    "os.makedirs(job_artifact_location, exist_ok=True)\n",
    "attrition_path = os.path.join(job_artifact_location, \"attrition-job1.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the published conda environment as inference and training conda environment. The published conda environment in this case resides in the bucket LAB_Conda and the path is given by  \"oci://LAB_Conda@ocuocictrng22/data-science-gmlv1_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./job-artifact/attrition-job1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {attrition_path}\n",
    "\n",
    "import ads\n",
    "import io\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pip\n",
    "import warnings\n",
    "import tempfile\n",
    "\n",
    "from ads.common.model import ADSModel\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.dataset.label_encoder import DataFrameLabelEncoder\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from collections import defaultdict\n",
    "from os import path\n",
    "from os.path import expanduser, join\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ads.model.framework.sklearn_model import SklearnModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ads.set_auth(\"resource_principal\")\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=15, n_classes=2, flip_y=0.05)\n",
    "trainx, testx, trainy, testy = train_test_split(X, y, test_size=30, random_state=42)\n",
    "\n",
    "\n",
    "sk_clf = RandomForestClassifier(random_state=42)\n",
    "sk_clf.fit(trainx, trainy)\n",
    "\n",
    "# Instantiate ads.model.framework.sklearn_model.SklearnModel\n",
    "sklearn_model = SklearnModel(\n",
    "    estimator=sk_clf, artifact_dir=tempfile.mkdtemp()\n",
    ")\n",
    "\n",
    "# Autogenerate score.py, serialized model, runtime.yaml, input_schema.json and output_schema.json\n",
    "artifact_dir = tempfile.mkdtemp()\n",
    "sklearn_model = SklearnModel(estimator=sk_clf, artifact_dir=artifact_dir)\n",
    "\n",
    "sklearn_model.prepare(\n",
    "    inference_conda_env=\"oci://LAB_Conda@ocuocictrng22/data-science-gmlv1_0\",\n",
    "    training_conda_env=\"oci://LAB_Conda@ocuocictrng22/data-science-gmlv1_0\" ,\n",
    "    use_case_type=UseCaseType.BINARY_CLASSIFICATION,\n",
    "    X_sample=trainx,\n",
    "    y_sample=trainy,\n",
    "    force_overwrite=True,\n",
    "    ignore_introspection=True\n",
    ")\n",
    "\n",
    "# Register scikit-learn model\n",
    "sklearn_model.save(display_name=\"Job-Sklearn-Model-Sept-25-2\", inference_conda_env=\"oci://conda-env-bucket@intoraclerohit/conda_environments/cpu/General Machine Learning for CPUs on Python 3.8/1.0/generalml_p38_cpu_v1\",ignore_introspection=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Job\n",
    "\n",
    "This section creates a [Data Science Job and a Job Run](https://docs.oracle.com/en-us/iaas/data-science/using/jobs-about.htm) using the ADS library.\n",
    "\n",
    "Using jobs, you can:\n",
    "\n",
    "* Run machine learning (ML) or data science tasks outside of your notebook sessions in JupyterLab.\n",
    "* Operationalize discrete data science and machine learning tasks as reusable runnable operations.\n",
    "* Automate your typical MLOps or CI/CD pipeline.\n",
    "* Execute batches or workloads triggered by events or actions.\n",
    "* Batch, mini-batch, or distributed batch job inference.\n",
    "\n",
    "Jobs are run in compute instances in the OCI Data Science service tenancy. The compute instance will run for the duration of your job and will automatically shut itself down at the completion of the job script.\n",
    "\n",
    "Output from the job can be captured using the OCI Logging service. While logging is optional, it is highly recommended. Without logging enabled, it is very difficult to troubleshoot job runs. The following cell will create a Log Group and Custom Log for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this cell more than once you will have to change the value of `job_name`, as it is used as the name of the Log Group and Log and they must have unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'Training-job-NOv09-23'\n",
    "log_group = OCILogGroup(display_name=job_name).create()\n",
    "log = log_group.create_log(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Job` class to create a job. The `.with_infrastructure()` method is used to define the default infrastructure that will be used. When a Job Run is created, many of the options can be changed. The Job Run will need to know what conda environment needs to be installed so that the script will execute. Generally, this will be the same conda environment that was used to develop and test the script. The Job Run needs to know the path of the script that is to be executed and the function to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = Job(job_name).with_infrastructure(\n",
    "    DataScienceJob().\\\n",
    "    with_shape_name(\"VM.Standard.E4.Flex\").\\\n",
    "    with_log_id(log.id).\\\n",
    "    with_log_group_id(log_group.id)).\\\n",
    "    with_runtime(ScriptRuntime().\\\n",
    "        with_source(\"job-artifact\", entrypoint=attrition_path).\\\n",
    "        with_custom_conda(\"oci://LAB_Conda@ocuocictrng22/data-science-gmlv1_0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the job object provides details about the job such as what conda environment it will use, logging information, what script will be run, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "kind: job\n",
       "spec:\n",
       "  infrastructure:\n",
       "    kind: infrastructure\n",
       "    spec:\n",
       "      jobType: DEFAULT\n",
       "      logGroupId: ocid1.loggroup.oc1.phx.amaaaaaas5adu2ia4ahx3m3ptxyi2zui2pmbygpnaukbt4x5tmhr76wu545a\n",
       "      logId: ocid1.log.oc1.phx.amaaaaaas5adu2iap6exjxeu6dakqfx6kyxdvi6apsaqp5vtlgvk3r2ymk6a\n",
       "      shapeName: VM.Standard.E4.Flex\n",
       "    type: dataScienceJob\n",
       "  name: Training-job-NOv09-23\n",
       "  runtime:\n",
       "    kind: runtime\n",
       "    spec:\n",
       "      conda:\n",
       "        type: published\n",
       "        uri: oci://LAB_Conda@ocuocictrng22/data-science-gmlv1_0\n",
       "      entrypoint: ./job-artifact/attrition-job1.py\n",
       "      scriptPathURI: job-artifact\n",
       "    type: script"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.create()` method to create the job. This will not trigger the execution of the job script. A job is a resource that contains the configuration and definition of the task to be executed while job runs are actual executions of a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsjob = job.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Job Run\n",
    "\n",
    "A Job allows for the definition of a template of a Job Run. A Job Run is the actual instance of the job being run. A Job can have many Job Runs. Further, the Job can be parameterized such that environment variables and command line arguments can be passed to the Job Run at run time. This allows for a single Job to define a family of Job Runs where each Job Run performs a slightly different action based on the environment variables and command line arguments. The Job Run used in this notebook is not parameterized as the goal is to demonstrate the basics of setting up a Job and creating a Job Run.\n",
    "\n",
    "The `.run()` method can be used to create a Job Run and execute the script. The `.watch()` method is used to watch the progress of the job. It displays information about the job run and the output of the job script. There is a slight difference between what is displayed in the `.watch()` method and what is in the logs. The `.watch()` method displays information about the setup and teardown of the Job Run. It also displays the output from the script itself. The log only captures the information from the execution of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: jobRun\n",
       "spec:\n",
       "  id: ocid1.datasciencejobrun.oc1.phx.amaaaaaas5adu2iagtqn6r6hdcku2cg262be5qb6pyewzolz4hdoig6ohdca\n",
       "  infrastructure:\n",
       "    kind: infrastructure\n",
       "    spec:\n",
       "      blockStorageSize: 50\n",
       "      compartmentId: ocid1.compartment.oc1..aaaaaaaabu6pgqbwe4are4ke7uzkq44rbvbnxwhybhmplialatq54kdvq4jq\n",
       "      displayName: Training-job-NOv09-23-run-2023-11-09-11:07.17\n",
       "      jobInfrastructureType: ME_STANDALONE\n",
       "      jobType: DEFAULT\n",
       "      logGroupId: ocid1.loggroup.oc1.phx.amaaaaaas5adu2ia4ahx3m3ptxyi2zui2pmbygpnaukbt4x5tmhr76wu545a\n",
       "      logId: ocid1.log.oc1.phx.amaaaaaas5adu2iap6exjxeu6dakqfx6kyxdvi6apsaqp5vtlgvk3r2ymk6a\n",
       "      projectId: ocid1.datascienceproject.oc1.phx.amaaaaaas5adu2iaa45m3uo4rgrpu4fzgpyltcsf4bn5grgpzlc675dmiprq\n",
       "      shapeConfigDetails:\n",
       "        memoryInGBs: 32.0\n",
       "        ocpus: 2.0\n",
       "      shapeName: VM.Standard.E4.Flex\n",
       "    type: dataScienceJob\n",
       "  name: Training-job-NOv09-23-run-2023-11-09-11:07.17\n",
       "  runtime:\n",
       "    kind: runtime\n",
       "    spec:\n",
       "      conda:\n",
       "        region: us-phoenix-1\n",
       "        type: published\n",
       "        uri: oci://LAB_Conda@ocuocictrng22/data-science-gmlv1_0\n",
       "      definedTags:\n",
       "        Oracle-Tags:\n",
       "          CreatedOn: '2023-11-09T11:07:16.960Z'\n",
       "      entrypoint: ./job-artifact/attrition-job1.py\n",
       "      scriptPathURI: job-artifact.zip\n",
       "    type: script"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsjob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The status of the job can be checked from the console</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:data-science-gmlv1_0_v1]",
   "language": "python",
   "name": "conda-env-data-science-gmlv1_0_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
