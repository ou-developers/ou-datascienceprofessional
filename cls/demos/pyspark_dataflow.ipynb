{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c2527b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# <font color=\"red\">Introduction to the Oracle Cloud Infrastructure Data Flow integration with Data Science</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "\n",
    "Oracle Cloud Infrastructure (OCI) Data Flow is a fully managed Apache Spark service that performs processing tasks on extremely large datasetsâ€”without infrastructure to deploy or manage. With Data Flow, you can configure Data Science notebooks to run applications interactively against Data Flow.\n",
    "\n",
    "Apache Spark is a distributed compute system designed to process data at scale. It supports large-scale SQL, batch, and stream processing, and machine learning tasks. Spark SQL provides database-like support. To query structured data, use Spark SQL. It is an ANSI standard SQL implementation.\n",
    "\n",
    "Apache Livy is a REST interface to Spark. Submit fault-tolerant Spark jobs from the notebook using synchronous and asynchronous methods to retrieve the output.\n",
    "\n",
    "SparkMagic allows for interactive communication with Spark using Livy. Using the `%%spark` magic directive within a JupyterLab code cell.\n",
    "\n",
    "Data Flow Sessions support auto-scaling Data Flow cluster capabilities.\n",
    "\n",
    "This notebook demonstrates how to run interactive Spark workloads on a long lasting [Oracle Cloud Infrastructure Data Flow](https://docs.oracle.com/en-us/iaas/data-flow/using/home.htm) cluster. **Data Flow Spark Magic** is used for interactively working with remote Spark clusters through Livy, a Spark REST server, in Jupyter notebooks. It includes a set of magic commands for interactively running Spark code.\n",
    "\n",
    "\n",
    "Developed on conda environment [PySpark 3.2 and Data Flow](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8\n",
    "\n",
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href='#pre-requisites'>1. Pre-requisites</a>\n",
    "    - <a href='#prerequisites_helpers'>1.1 Helpers</a>\n",
    "    - <a href='#prerequisites_authentication'>1.2 Authentication</a>\n",
    "    - <a href='#prerequisites_variables'>1.3 Variables</a>    \n",
    "- <a href='#dataflow_magic'>2. Data Flow Spark Magic</a>\n",
    "    - <a href='#load_extension'>2.1. Load Extension</a>\n",
    "    - <a href='#create_session'>2.2. Create Session</a>\n",
    "        - <a href='#create_session_dynamic_allocation'>2.2.1. Example command for Spark dynamic allocation</a>\n",
    "    - <a href='#update_session'>2.3. Update Session</a>\n",
    "    - <a href='#stop_session'>2.4. Stop Session</a>\n",
    "- <a href='#basic_examples'>3. Basic Spark Usage Examples</a>\n",
    "    - <a href='#examples_pyspark'>3.1. PySpark</a>\n",
    "    - <a href='#examples_sparksql'>3.2. Spark SQL</a>\n",
    "- <a href='#cleanup'>4. Clean Up</a> \n",
    "- <a href='#ref'>5. References</a>   \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c860d8",
   "metadata": {},
   "source": [
    "<a id='pre-requisites'></a>\n",
    "# 1. Pre-requisites \n",
    "\n",
    "Data Flow Sessions are accessible through the following conda environment: \n",
    "\n",
    "* **PySpark 3.2 and Data Flow 2.0 (pyspark32_p38_cpu_v2)**\n",
    "\n",
    "You can customize `pypspark32_p38_cpu_v2`, publish it, and use it as a runtime environment for a Data Flow Session. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce663efb",
   "metadata": {},
   "source": [
    "<a id=\"prerequisites_helpers\"></a>\n",
    "## 1.1 Helpers\n",
    "This section provides a helper method used across the notebook to prepare arguments for the magic commands. This function is particularly useful when you want to pass Python variables as arguments to the spark magic commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70074cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prepare_command(command: dict) -> str:\n",
    "    \"\"\"Converts dictionary command to the string formatted commands.\"\"\"\n",
    "    return f\"'{json.dumps(command)}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9517190",
   "metadata": {},
   "source": [
    "<a id=\"prerequisites_authentication\"></a>\n",
    "## 1.2. Authentication\n",
    "The [Oracle Accelerated Data Science SDK (ADS)](https://docs.oracle.com/iaas/tools/ads-sdk/latest/index.html) controls the authentication mechanism with the Data Flow Session Spark cluster.<br> \n",
    "To setup authentication use the ```ads.set_auth(\"resource_principal\")``` or ```ads.set_auth(\"api_key\")```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89ee3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "\n",
    "ads.set_auth(\"resource_principal\")  # Supported values: resource_principal, api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9887ba9",
   "metadata": {},
   "source": [
    "<a id=\"prerequisites_variables\"></a>\n",
    "## 1.3. Variables\n",
    "To run this notebook, you must provide some information about your tenancy configuration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efc2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "compartment_id = os.environ.get(\"NB_SESSION_COMPARTMENT_OCID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4faa3",
   "metadata": {},
   "source": [
    "<a id=\"dataflow_magic\"></a>\n",
    "# 2. Data Flow Spark Magic\n",
    "Data Flow Spark Magic commands allow you to interactively work with Data Flow Spark clusters (sessions) in Jupyter notebooks through the Livy REST API. It provides a set of Jupyter Notebook cell magic commands to turn Jupyter into an integrated Spark development environment for remote clusters. \n",
    "\n",
    "**Data Flow Magic allows you to:**\n",
    "\n",
    "* Run Spark code against Data Flow remote Spark cluster\n",
    "* Create a Data Flow Spark Session with SparkContext against Data Flow remote Spark cluster\n",
    "* Capture the output of Spark queries as a local Pandas data frame to interact easily with other Python libraries (e.g. matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7145c",
   "metadata": {},
   "source": [
    "<a id=\"load_extension\"></a>\n",
    "### 2.1. Load Spark Magic Commands and Getting Help\n",
    "Data Flow Spark Magic is a JupyterLab extension that you need to activate in your notebook using the `%load_ext dataflow.magics` magic command.<br>\n",
    "After the extension is activated, the `%help` command can be used to get the list of supported commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f5df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845c496",
   "metadata": {},
   "source": [
    "The `%help` command gives you a list of all the available commands, along with a list of their arguments and example calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e61bc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th style=\"text-align:left\">Magic</th>\n",
       "    <th style=\"text-align:left\">Example</th>\n",
       "    <th style=\"text-align:left\">Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>create_session</td>\n",
       "    <td  style=\"text-align:left\">%create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard2.1\",\"executorShape\":\"VM.Standard2.1\",\"numExecutors\":1,\"archiveUri\":\"Object Storage URL for Data Flow zip archive.\",\"metastoreId\":\"optional metastore OCID\",\"configuration\":{      \"spark.archives\":\"oci://bucket@namespace/path/to/conda/pack\",      #optional property to use Dataflow 'Run' resource to access OCI resources.\n",
       "      \"dataflow.auth\":\"resource_principal\"   }}'</td>\n",
       "    <td style=\"text-align:left\">Creates new session by providing session details.<br/><br/><b>Example command for Flex shapes :</b><br/>\n",
       "    %create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard.E4.Flex\",\"executorShape\":\"VM.Standard.E4.Flex\",\"numExecutors\":1,\"driverShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16},\"executorShapeConfig\":{\"ocpus\":1,\"memoryInGBs\":16}}'\n",
       "    <br/><br/><b>Example command for Spark dynamic allocation :</b><br/>\n",
       "    %create_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"sparkVersion\":\"3.2.1\",\"driverShape\":\"VM.Standard2.1\",\"executorShape\":\"VM.Standard2.1\",\"numExecutors\":1,\"configuration\":{ \"spark.dynamicAllocation.enabled\":\"true\", \"spark.dynamicAllocation.shuffleTracking.enabled\":\"true\", \"spark.dynamicAllocation.minExecutors\":\"1\", \"spark.dynamicAllocation.maxExecutors\":\"4\", \"spark.dynamicAllocation.executorIdleTimeout\":\"60\", \"spark.dynamicAllocation.schedulerBacklogTimeout\":\"60\", \"spark.dataflow.dynamicAllocation.quotaPolicy\":\"min\" }}'\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>activate_session</td>\n",
       "    <td  style=\"text-align:left\">%activate_session -l python -c '{\"compartmentId\":\"Data Flow Run resource compartment OCID\",\"displayName\":\"SessionApp\",\"applicationId\":\"Existing sessionId to activate.\"}'</td>\n",
       "    <td  style=\"text-align:left\">Activate session by providing existing sessionId.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>use_session</td>\n",
       "    <td  style=\"text-align:left\">%use_session -s {sessionId}</td>\n",
       "    <td  style=\"text-align:left\">To use already existing active session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>status</td>\n",
       "    <td  style=\"text-align:left\">%status</td>\n",
       "    <td  style=\"text-align:left\">Outputs current session status.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>update_session</td>\n",
       "    <td  style=\"text-align:left\">%update_session -i '{\"maxDurationInMinutes\": 4896,\"idleTimeoutInMinutes\": 4888}'</td>\n",
       "    <td  style=\"text-align:left\">Updates current active session[not session config] for max duration or idle time out.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>stop_session</td>\n",
       "    <td  style=\"text-align:left\">%stop_session</td>\n",
       "    <td  style=\"text-align:left\">Stops current active session. One active session should be associated with current notebook to stop.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>config</td>\n",
       "    <td  style=\"text-align:left\">%config</td>\n",
       "    <td  style=\"text-align:left\">Outputs current session configuration.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure_session</td>\n",
       "    <td  style=\"text-align:left\">%configure_session -i '{\"driverShape\": \"VM.Standard2.1\", \"executorShape\": \"VM.Standard2.1\", \"numExecutors\": 1}'</td>\n",
       "    <td  style=\"text-align:left\">Configures the session creation parameters. The force flag -f is mandatory for immediate effect of the config change, in that case session will be dropped and recreated.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td  style=\"text-align:left\">%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td  style=\"text-align:left\">Executes spark commands.<br/>\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3638bd",
   "metadata": {},
   "source": [
    "If you want to access the docstrings of any magic command and figure out what arguments to provide, simply add `?` at then end of the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4935a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %create_session [-l LANGUAGE] [-c CONFIG] [-e ENDPOINT]\n",
       "\n",
       "optional arguments:\n",
       "  -l LANGUAGE, --language LANGUAGE\n",
       "                        Language to use.\n",
       "  -c CONFIG, --config CONFIG\n",
       "                        Region to use.\n",
       "  -e ENDPOINT, --endpoint ENDPOINT\n",
       "                        Endpoint to use.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/conda/pyspark32_p38_cpu_v2/lib/python3.8/site-packages/dataflow/livyclientlib/exceptions.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?%create_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99723c",
   "metadata": {},
   "source": [
    "### 2.2. Create Session\n",
    "To create a new Data Flow cluster session use the `%create_session` magic command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304e50c",
   "metadata": {},
   "source": [
    "<a id=\"create_session_dynamic_allocation\"></a>\n",
    "#### 2.2.1. Example command for Spark dynamic allocation (aka auto-scaling)\n",
    "To help you save resources and reduce time on management, Spark [dynamic allocation](https://docs.oracle.com/iaas/data-flow/using/dynamic-alloc-about.htm#dynamic-alloc-about) is now enabled in Data Flow.\n",
    "\n",
    "Resource planning for data processing is a complex task. Resource usage is a function of the volume of the data. Day-to-day volumes of data can vary, meaning the computational resource required changes, too.\n",
    "\n",
    "You can define a Data Flow cluster based on a range of executors, instead of just a fixed number of executors. Spark provides a mechanism to dynamically adjust the resources application occupies based on the workload. The application might relinquish resources if they are no longer used and request them again later when there is demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80babac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Cluster..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f1ce4583694c2da1dc69fdf23d93ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster setup is still in progress.\n",
      "Cluster is ready..\n",
      "Starting Spark application..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Session ID</th><th>Kind</th><th>State</th><th>Current session</th></tr><tr><td>ocid1.dataflowapplication.oc1.iad.anuwcljsnf25m3qa5pivohxxhjhehupxfabvsfmkekct7ovtccfanjd4dfiq</td><td>pyspark</td><td>IN_PROGRESS</td><td><a target=\"_blank\" href=\"https://console.us-phoenix-1.oraclecloud.com/data-flow/runs/details/ocid1.dataflowrun.oc1.iad.anuwcljsnf25m3qasn2y3djaswfpg754q4zdodor32aaoif6vvfoq5ocyadq?region=us-ashburn-1\">Dataflow Run</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "SparkContext available as 'sc'.\n"
     ]
    }
   ],
   "source": [
    "command = prepare_command(\n",
    "    {\n",
    "        \"compartmentId\": compartment_id,\n",
    "        \"displayName\": \"TestDataFLowSessionDynamicAllocation\",\n",
    "        \"language\": \"PYTHON\",\n",
    "        \"sparkVersion\": \"3.2.1\",\n",
    "        \"numExecutors\": 2,\n",
    "        \"driverShape\": \"VM.Standard.E4.Flex\",\n",
    "        \"executorShape\": \"VM.Standard.E4.Flex\",\n",
    "        \"driverShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 32},\n",
    "        \"executorShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 32},\n",
    "        \"configuration\": {\n",
    "            \"fs.oci.client.hostname\": \"https://objectstorage.us-ashburn-1.oraclecloud.com\",\n",
    "            \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "            \"spark.dynamicAllocation.shuffleTracking.enabled\": \"true\",\n",
    "            \"spark.dynamicAllocation.minExecutors\": \"1\",\n",
    "            \"spark.dynamicAllocation.maxExecutors\": \"4\",\n",
    "            \"spark.dynamicAllocation.executorIdleTimeout\": \"60\",\n",
    "            \"spark.dynamicAllocation.schedulerBacklogTimeout\": \"60\",\n",
    "            \"spark.dataflow.dynamicAllocation.quotaPolicy\": \"min\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "%create_session -l python -c $command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e3970",
   "metadata": {},
   "source": [
    "Use the `%status` magic command to check the status of the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e8993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Session</th><th>State</th><th>Max Duration In Minutes</th><th>Total Execution Time In Minutes</th><th>Remaining Duration In Minutes</th><th>Current Session</th></tr><tr><td>ocid1.dataflowapplication.oc1.iad.anuwcljsnf25m3qa5pivohxxhjhehupxfabvsfmkekct7ovtccfanjd4dfiq</td><td>IN_PROGRESS</td><td>1440</td><td>18</td><td>1422</td><td><a target=\"_blank\" href=\"https://console.us-phoenix-1.oraclecloud.com/data-flow/runs/details/ocid1.dataflowrun.oc1.iad.anuwcljsnf25m3qasn2y3djaswfpg754q4zdodor32aaoif6vvfoq5ocyadq?region=us-ashburn-1\">Dataflow Run</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0d0c0",
   "metadata": {},
   "source": [
    "Use the `%config` magic command to see the configuration of the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efbb1725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"applicationLogConfig\": {\n",
      "        \"logGroupId\": null,\n",
      "        \"logId\": null\n",
      "    },\n",
      "    \"archiveUri\": \"\",\n",
      "    \"arguments\": null,\n",
      "    \"className\": \"\",\n",
      "    \"compartmentId\": \"ocid1.compartment.oc1..aaaaaaaa3vneejwlpdacc55zo33foeu2twbykcalr6rhvjxysoyvoflrubda\",\n",
      "    \"configuration\": {\n",
      "        \"fs.oci.client.hostname\": \"https://objectstorage.us-ashburn-1.oraclecloud.com\",\n",
      "        \"spark.dataflow.dynamicAllocation.quotaPolicy\": \"min\",\n",
      "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
      "        \"spark.dynamicAllocation.executorIdleTimeout\": \"60\",\n",
      "        \"spark.dynamicAllocation.maxExecutors\": \"4\",\n",
      "        \"spark.dynamicAllocation.minExecutors\": \"1\",\n",
      "        \"spark.dynamicAllocation.schedulerBacklogTimeout\": \"60\",\n",
      "        \"spark.dynamicAllocation.shuffleTracking.enabled\": \"true\"\n",
      "    },\n",
      "    \"definedTags\": {\n",
      "        \"Owner\": {\n",
      "            \"CreatedOn\": \"2023-05-11T00:27:03.507Z\",\n",
      "            \"Creator\": \"ocid1.datasciencenotebooksession.oc1.iad.amaaaaaanf25m3qaf74n5jqyszzlhxypcjputesanccwjixpszt4sncpx5ha\",\n",
      "            \"PrincipalType\": \"resource\"\n",
      "        }\n",
      "    },\n",
      "    \"description\": null,\n",
      "    \"displayName\": \"TestDataFLowSessionDynamicAllocation\",\n",
      "    \"driverShape\": \"VM.Standard.E4.Flex\",\n",
      "    \"driverShapeConfig\": {\n",
      "        \"memoryInGBs\": 32,\n",
      "        \"ocpus\": 2\n",
      "    },\n",
      "    \"execute\": null,\n",
      "    \"executorShape\": \"VM.Standard.E4.Flex\",\n",
      "    \"executorShapeConfig\": {\n",
      "        \"memoryInGBs\": 32,\n",
      "        \"ocpus\": 2\n",
      "    },\n",
      "    \"fileUri\": \"\",\n",
      "    \"freeformTags\": {},\n",
      "    \"id\": \"ocid1.dataflowapplication.oc1.iad.anuwcljsnf25m3qa5pivohxxhjhehupxfabvsfmkekct7ovtccfanjd4dfiq\",\n",
      "    \"idleTimeoutInMinutes\": 1440,\n",
      "    \"lakeId\": \"\",\n",
      "    \"language\": \"PYTHON\",\n",
      "    \"lifecycleState\": \"ACTIVE\",\n",
      "    \"logsBucketUri\": \"oci://dataflow-logs@intoraclerohit/\",\n",
      "    \"maxDurationInMinutes\": 1440,\n",
      "    \"metastoreId\": null,\n",
      "    \"numExecutors\": 2,\n",
      "    \"ownerPrincipalId\": \"ocid1.datasciencenotebooksession.oc1.iad.amaaaaaanf25m3qaf74n5jqyszzlhxypcjputesanccwjixpszt4sncpx5ha\",\n",
      "    \"ownerUserName\": \"ocid1.datasciencenotebooksession.oc1.iad.amaaaaaanf25m3qaf74n5jqyszzlhxypcjputesanccwjixpszt4sncpx5ha\",\n",
      "    \"parameters\": null,\n",
      "    \"poolId\": null,\n",
      "    \"privateEndpointId\": null,\n",
      "    \"sparkVersion\": \"3.2.1\",\n",
      "    \"timeCreated\": \"2023-05-11T00:27:04.542Z\",\n",
      "    \"timeUpdated\": \"2023-05-11T00:27:04.542Z\",\n",
      "    \"type\": \"SESSION\",\n",
      "    \"warehouseBucketUri\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522eab8",
   "metadata": {},
   "source": [
    "<a id=\"update_session\"></a>\n",
    "### 2.3. Update Session\n",
    "You can modify the configuration of your running session using the `%update_session` command. For example, Data Flow Sessions can last up to 7 days or 10080 mins (168 hours) (**maxDurationInMinutes**) and have default idle timeout value of 480 mins (8 hours)(**idleTimeoutInMinutes**). Here's how you would update your session and configure for a max session time:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1cee20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Max Duration In Minutes</th><th>Idle Timeout In Minutes</th></tr><tr><td>1440</td><td>420</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%update_session -i '{\"maxDurationInMinutes\": 1440, \"idleTimeoutInMinutes\": 420}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa66bd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Session</th><th>State</th><th>Max Duration In Minutes</th><th>Total Execution Time In Minutes</th><th>Remaining Duration In Minutes</th><th>Current Session</th></tr><tr><td>ocid1.dataflowapplication.oc1.iad.anuwcljsnf25m3qa5pivohxxhjhehupxfabvsfmkekct7ovtccfanjd4dfiq</td><td>IN_PROGRESS</td><td>1440</td><td>40</td><td>1400</td><td><a target=\"_blank\" href=\"https://console.us-phoenix-1.oraclecloud.com/data-flow/runs/details/ocid1.dataflowrun.oc1.iad.anuwcljsnf25m3qasn2y3djaswfpg754q4zdodor32aaoif6vvfoq5ocyadq?region=us-ashburn-1\">Dataflow Run</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f09c2b",
   "metadata": {},
   "source": [
    "<a id=\"stop_session\"></a>\n",
    "### 2.4. Stop Session\n",
    "To stop the current session use the `%stop_session` magic command. You don't need to provide any arguments for this command. The current active cluster will be stopped. All data in memory will be lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29925b",
   "metadata": {},
   "source": [
    "<a id=\"basic_examples\"></a>\n",
    "# 3. Basic Spark Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4428769",
   "metadata": {},
   "source": [
    "A SparkContext (`sc`) and HiveContext (`sqlContext`) are automatically created in the session cluster. The magic commands include the `%%spark` command to run Spark commands in the cluster. You can access information about the Spark application, define a dataframe where results are to be stored, modify the configuration, and so on.\n",
    "\n",
    "The `%%spark` magic command comes with a number of parameters that allow you to interact with the Data Flow Spark cluster. **Any cell content that starts with the `%%spark` command will be executed in the remote Spark cluster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9247bb2",
   "metadata": {},
   "source": [
    "<a id=\"examples_pyspark\"></a>\n",
    "### 3.1. PySpark\n",
    "The `sc` variable represents the Spark context and it's available when the `%%spark` magic command is used. The next cell is a toy example of how to use `sc` in a Data Flow Spark Magic cell. The cell calls the [`.parallelize()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html) method, which creates an [RDD](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html), `numbers`, from a list of numbers. Information about the RDD is printed. The [`.toDebugString()`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.RDD.toDebugString.html) method returns a description of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039ed12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2716903e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 4\n",
      "The RDD, numbers, has the following description\n",
      "b'(4) ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([4, 3, 2, 1])\n",
    "print(f\"First element of numbers is {numbers.first()}\")\n",
    "print(f\"The RDD, numbers, has the following description\\n{numbers.toDebugString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec9bdf",
   "metadata": {},
   "source": [
    "<a id=\"examples_sparksql\"></a>\n",
    "### 3.2. Spark SQL\n",
    "Using the `-c sql` option allows you to run Spark SQL commands in a cell. In this section, the [NYC Taxi and Limousine Commission (TLC) Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) dataset is used. The size of the dataset is around **35GB**. \n",
    "\n",
    "The next cell reads the dataset into a Spark dataframe, and then saves it as a view used to demonstrate Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b22734bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|          pickup_at|         dropoff_at|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|      CMT|2011-01-29 02:38:35|2011-01-29 02:47:07|              1|          1.2|       -74.00526|      40.729084|           1|                 N|        -73.98869|       40.727127|         CSH|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|\n",
      "|      CMT|2011-01-28 10:38:19|2011-01-28 10:42:18|              1|          0.4|       -73.96858|       40.75917|           1|                 N|        -73.96433|       40.764664|         CSH|        4.1|  0.0|    0.5|       0.0|         0.0|         4.6|\n",
      "|      CMT|2011-01-28 23:49:58|2011-01-28 23:57:44|              3|          1.2|       -73.98071|       40.74239|           1|                 N|        -73.98703|        40.72953|         CSH|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|\n",
      "|      CMT|2011-01-28 23:52:09|2011-01-28 23:59:21|              3|          0.8|      -73.993774|       40.74733|           1|                 N|        -73.99138|        40.75005|         CSH|        5.3|  0.5|    0.5|       0.0|         0.0|         6.3|\n",
      "|      CMT|2011-01-28 10:34:39|2011-01-28 11:25:50|              1|          5.3|       -73.99148|      40.749935|           1|                 N|        -73.95023|       40.775627|         CSH|       25.3|  0.0|    0.5|       0.0|         0.0|        25.8|\n",
      "|      CMT|2011-01-28 23:50:00|2011-01-28 23:58:11|              2|          1.2|       -73.95035|      40.806408|           1|                 N|        -73.96071|       40.818275|         CSH|        6.5|  0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|      CMT|2011-01-29 02:38:48|2011-01-29 02:50:37|              1|          2.7|       -73.95197|      40.777428|           1|                 N|       -73.982155|        40.76134|         CSH|        9.3|  0.5|    0.5|       0.0|         0.0|        10.3|\n",
      "|      CMT|2011-01-29 02:41:16|2011-01-29 02:45:45|              2|          0.9|       -73.94498|       40.71823|           1|                 N|         -73.9569|       40.712723|         CSH|        4.9|  0.5|    0.5|       0.0|         0.0|         5.9|\n",
      "|      CMT|2011-01-28 23:50:51|2011-01-29 00:07:55|              1|          3.3|       -73.98991|      40.756542|           1|                 N|        -73.95675|       40.778217|         CSH|       12.1|  0.5|    0.5|       0.0|         0.0|        13.1|\n",
      "|      CMT|2011-01-29 02:41:34|2011-01-29 03:08:14|              1|          7.2|       -73.95892|       40.81009|           1|                 N|        -73.99513|       40.728607|         CSH|       20.5|  0.5|    0.5|       0.0|         0.0|        21.5|\n",
      "|      CMT|2011-01-28 23:50:22|2011-01-29 00:03:23|              1|          2.4|       -73.99417|       40.72269|           1|                 N|        -74.00063|        40.74783|         CSH|        9.3|  0.5|    0.5|       0.0|         0.0|        10.3|\n",
      "|      CMT|2011-01-29 02:40:30|2011-01-29 02:43:08|              2|          0.5|       -73.98601|      40.726665|           1|                 N|        -73.97861|        40.72865|         CSH|        3.7|  0.5|    0.5|       0.0|         0.0|         4.7|\n",
      "|      CMT|2011-01-29 02:42:47|2011-01-29 02:50:31|              1|          1.0|       -73.99766|      40.721508|           1|                 N|        -74.00639|       40.721733|         CSH|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|\n",
      "|      CMT|2011-01-28 23:51:10|2011-01-29 00:03:19|              1|          1.5|      -73.963135|       40.75675|           1|                 N|         -73.9885|        40.76574|         CSH|        8.1|  0.5|    0.5|       0.0|         0.0|         9.1|\n",
      "|      CMT|2011-01-28 05:07:16|2011-01-28 05:12:25|              1|          0.7|       -73.97815|      40.729393|           1|                 N|        -73.97816|       40.720757|         CSH|        4.5|  0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "|      CMT|2011-01-29 02:42:31|2011-01-29 02:55:56|              1|          0.8|      -73.982605|      40.723316|           1|                 N|       -73.988205|       40.731136|         CSH|        8.1|  0.5|    0.5|       0.0|         0.0|         9.1|\n",
      "|      CMT|2011-01-28 23:51:01|2011-01-28 23:59:06|              2|          1.7|       -73.99149|       40.75524|           1|                 N|       -73.986275|       40.737213|         CSH|        6.5|  0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|      CMT|2011-01-29 02:39:23|2011-01-29 02:59:31|              1|          3.6|       -73.96349|      40.765736|           1|                 N|       -73.995415|        40.73358|         CSH|       12.9|  0.5|    0.5|       0.0|         0.0|        13.9|\n",
      "|      CMT|2011-01-29 02:41:18|2011-01-29 02:50:43|              2|          2.0|       -73.98452|      40.768402|           1|                 N|        -73.99172|       40.744762|         CSH|        7.7|  0.5|    0.5|       0.0|         0.0|         8.7|\n",
      "|      CMT|2011-01-28 10:30:44|2011-01-28 10:48:05|              1|          2.1|      -73.959366|       40.77083|           1|                 N|       -73.959366|        40.77083|         CSH|       10.5|  0.0|    0.5|       0.0|         0.0|        11.0|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "df_nyc_tlc = spark.read.parquet(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/nyc_tlc/201[1,2,3,4,5,6,7,8]/**/data.parquet\", header=False, inferSchema=True)\n",
    "df_nyc_tlc.show()\n",
    "\n",
    "df_nyc_tlc.createOrReplaceTempView(\"nyc_tlc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b3ce0",
   "metadata": {},
   "source": [
    "The following cell uses the `-c sql` option to tell Data Flow Spark Magic that the contents of the cell is SparkSQL. The `-o <variable>` option takes the results of the Spark SQL operation and stores it in the defined variable. In this case, the `df_nyc_tlc` will be a Pandas dataframe that is available to be used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60817529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMT</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMT</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>CRD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>CRD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>CRD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>CRD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>6.8</td>\n",
       "      <td>CRD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    vendor_id  passenger_count  trip_distance payment_type\n",
       "0         CMT                1            1.2          CSH\n",
       "1         CMT                1            0.4          CSH\n",
       "2         CMT                3            1.2          CSH\n",
       "3         CMT                3            0.8          CSH\n",
       "4         CMT                1            5.3          CSH\n",
       "..        ...              ...            ...          ...\n",
       "995       CMT                1            0.5          CRD\n",
       "996       CMT                2            7.2          CRD\n",
       "997       CMT                1            4.7          CRD\n",
       "998       CMT                1            0.3          CRD\n",
       "999       CMT                1            6.8          CRD\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql -o df_nyc_tlc\n",
    "SELECT vendor_id, passenger_count, trip_distance, payment_type FROM nyc_tlc LIMIT 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c235aabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_nyc_tlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e96c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMT</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMT</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vendor_id  passenger_count  trip_distance payment_type\n",
       "0       CMT                1            1.2          CSH\n",
       "1       CMT                1            0.4          CSH\n",
       "2       CMT                3            1.2          CSH\n",
       "3       CMT                3            0.8          CSH\n",
       "4       CMT                1            5.3          CSH"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc_tlc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b30ab844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>nyc_tlc</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace tableName  isTemporary\n",
       "0       NaT   nyc_tlc         True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql -o df_tables\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8f58d",
   "metadata": {},
   "source": [
    "Similarly you can use `sqlContext` to query the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e26dc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|          pickup_at|         dropoff_at|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|      CMT|2011-01-29 02:38:35|2011-01-29 02:47:07|              1|          1.2|       -74.00526|      40.729084|           1|                 N|        -73.98869|       40.727127|         CSH|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|\n",
      "|      CMT|2011-01-28 10:38:19|2011-01-28 10:42:18|              1|          0.4|       -73.96858|       40.75917|           1|                 N|        -73.96433|       40.764664|         CSH|        4.1|  0.0|    0.5|       0.0|         0.0|         4.6|\n",
      "|      CMT|2011-01-28 23:49:58|2011-01-28 23:57:44|              3|          1.2|       -73.98071|       40.74239|           1|                 N|        -73.98703|        40.72953|         CSH|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|\n",
      "|      CMT|2011-01-28 23:52:09|2011-01-28 23:59:21|              3|          0.8|      -73.993774|       40.74733|           1|                 N|        -73.99138|        40.75005|         CSH|        5.3|  0.5|    0.5|       0.0|         0.0|         6.3|\n",
      "|      CMT|2011-01-28 10:34:39|2011-01-28 11:25:50|              1|          5.3|       -73.99148|      40.749935|           1|                 N|        -73.95023|       40.775627|         CSH|       25.3|  0.0|    0.5|       0.0|         0.0|        25.8|\n",
      "|      CMT|2011-01-28 23:50:00|2011-01-28 23:58:11|              2|          1.2|       -73.95035|      40.806408|           1|                 N|        -73.96071|       40.818275|         CSH|        6.5|  0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|      CMT|2011-01-29 02:38:48|2011-01-29 02:50:37|              1|          2.7|       -73.95197|      40.777428|           1|                 N|       -73.982155|        40.76134|         CSH|        9.3|  0.5|    0.5|       0.0|         0.0|        10.3|\n",
      "|      CMT|2011-01-29 02:41:16|2011-01-29 02:45:45|              2|          0.9|       -73.94498|       40.71823|           1|                 N|         -73.9569|       40.712723|         CSH|        4.9|  0.5|    0.5|       0.0|         0.0|         5.9|\n",
      "|      CMT|2011-01-28 23:50:51|2011-01-29 00:07:55|              1|          3.3|       -73.98991|      40.756542|           1|                 N|        -73.95675|       40.778217|         CSH|       12.1|  0.5|    0.5|       0.0|         0.0|        13.1|\n",
      "|      CMT|2011-01-29 02:41:34|2011-01-29 03:08:14|              1|          7.2|       -73.95892|       40.81009|           1|                 N|        -73.99513|       40.728607|         CSH|       20.5|  0.5|    0.5|       0.0|         0.0|        21.5|\n",
      "|      CMT|2011-01-28 23:50:22|2011-01-29 00:03:23|              1|          2.4|       -73.99417|       40.72269|           1|                 N|        -74.00063|        40.74783|         CSH|        9.3|  0.5|    0.5|       0.0|         0.0|        10.3|\n",
      "|      CMT|2011-01-29 02:40:30|2011-01-29 02:43:08|              2|          0.5|       -73.98601|      40.726665|           1|                 N|        -73.97861|        40.72865|         CSH|        3.7|  0.5|    0.5|       0.0|         0.0|         4.7|\n",
      "|      CMT|2011-01-29 02:42:47|2011-01-29 02:50:31|              1|          1.0|       -73.99766|      40.721508|           1|                 N|        -74.00639|       40.721733|         CSH|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|\n",
      "|      CMT|2011-01-28 23:51:10|2011-01-29 00:03:19|              1|          1.5|      -73.963135|       40.75675|           1|                 N|         -73.9885|        40.76574|         CSH|        8.1|  0.5|    0.5|       0.0|         0.0|         9.1|\n",
      "|      CMT|2011-01-28 05:07:16|2011-01-28 05:12:25|              1|          0.7|       -73.97815|      40.729393|           1|                 N|        -73.97816|       40.720757|         CSH|        4.5|  0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "|      CMT|2011-01-29 02:42:31|2011-01-29 02:55:56|              1|          0.8|      -73.982605|      40.723316|           1|                 N|       -73.988205|       40.731136|         CSH|        8.1|  0.5|    0.5|       0.0|         0.0|         9.1|\n",
      "|      CMT|2011-01-28 23:51:01|2011-01-28 23:59:06|              2|          1.7|       -73.99149|       40.75524|           1|                 N|       -73.986275|       40.737213|         CSH|        6.5|  0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|      CMT|2011-01-29 02:39:23|2011-01-29 02:59:31|              1|          3.6|       -73.96349|      40.765736|           1|                 N|       -73.995415|        40.73358|         CSH|       12.9|  0.5|    0.5|       0.0|         0.0|        13.9|\n",
      "|      CMT|2011-01-29 02:41:18|2011-01-29 02:50:43|              2|          2.0|       -73.98452|      40.768402|           1|                 N|        -73.99172|       40.744762|         CSH|        7.7|  0.5|    0.5|       0.0|         0.0|         8.7|\n",
      "|      CMT|2011-01-28 10:30:44|2011-01-28 10:48:05|              1|          2.1|      -73.959366|       40.77083|           1|                 N|       -73.959366|        40.77083|         CSH|       10.5|  0.0|    0.5|       0.0|         0.0|        11.0|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "df_nyc_tlc = sqlContext.sql(\"SELECT * FROM nyc_tlc LIMIT 1000\")\n",
    "df_nyc_tlc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f8248",
   "metadata": {},
   "source": [
    "<a id='cleanup'></a>\n",
    "# 4. Clean Up\n",
    "Use the `%stop_session` magic command to stop your active Data Flow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474d032",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# 5. References\n",
    "\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)\n",
    "- [Data Flow Policies](https://docs.oracle.com/iaas/data-flow/using/policies.htm/)\n",
    "- [Getting Started with Data Flow](https://docs.oracle.com/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "- [About Data Science Policies](https://docs.oracle.com/iaas/data-science/using/policies.htm)\n",
    "- [Data Catalog Metastore](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
